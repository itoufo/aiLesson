<img src="/docs/images/common/haiia_logo.png" alt="一般社団法人 健全AI教育協会" width="280">

# 第2章：生成AIの仕組み

## 学習目標

この章を終えると、以下のことができるようになります：

- 生成AIの基本的な動作原理を説明できる
- Transformerアーキテクチャの概要を理解できる
- 「ハルシネーション」などの生成AIの限界を説明できる
- 生成AIの出力を適切に評価・検証できる

---

## 2.1 機械学習の基礎

### 2.1.1 機械学習とは

**機械学習（Machine Learning）** は、データからパターンを学習し、予測や判断を行う技術です。

![従来のプログラミング vs 機械学習](/docs/images/01_入門/02_ml_vs_programming.jpeg)

### 2.1.2 学習の種類

| 種類 | 説明 | 例 |
|------|------|-----|
| **教師あり学習** | 正解データを使って学習 | スパム判定、画像分類 |
| **教師なし学習** | 正解なしでパターンを発見 | クラスタリング、異常検知 |
| **強化学習** | 報酬を最大化するよう学習 | ゲームAI、ロボット制御 |
| **自己教師あり学習** | データ自体から学習信号を生成 | LLMの事前学習 |

---

## 2.2 ニューラルネットワークと深層学習

### 2.2.1 ニューラルネットワークの構造

![ニューラルネットワークの構造](/docs/images/01_入門/02_neural_network.jpeg)

### 2.2.2 深層学習（ディープラーニング）

**深層学習**は、多くの隠れ層を持つニューラルネットワークです。

![深層学習の特徴抽出](/docs/images/01_入門/02_deep_learning.jpeg)

**特徴**:
- 層が深くなるほど複雑なパターンを学習
- 大量のデータと計算リソースが必要
- 特徴量を自動で抽出（人間による設計が不要）

---

## 2.3 大規模言語モデル（LLM）の仕組み

### 2.3.1 Transformerアーキテクチャ

2017年に発表された**Transformer**は、現在の生成AIの基盤技術です。

![Transformer自己注意機構](/docs/images/01_入門/02_transformer.jpeg)

### 2.3.2 LLMの学習プロセス

![LLMの学習プロセス](/docs/images/01_入門/02_llm_training.jpeg)

### 2.3.3 テキスト生成の仕組み

![テキスト生成の仕組み](/docs/images/01_入門/02_text_generation.jpeg)

---

## 2.4 生成AIの限界と注意点

### 2.4.1 ハルシネーション（幻覚）

**ハルシネーション**とは、AIが事実と異なる情報をあたかも正しいかのように生成する現象です。

![ハルシネーションの例](/docs/images/01_入門/02_hallucination.jpeg)

**なぜ起きるか**:
- LLMは「もっともらしい」文章を生成するよう学習している
- 事実の正確性を直接学習していない
- 学習データにない情報を「補完」しようとする

### 2.4.2 その他の限界

| 限界 | 説明 |
|------|------|
| **知識のカットオフ** | 学習データ以降の情報を知らない |
| **計算・推論の限界** | 複雑な数学計算や論理推論でミスする |
| **文脈長の制限** | 一度に処理できるテキスト量に上限がある |
| **バイアスの反映** | 学習データに含まれる偏見を反映する |
| **引用・出典の不正確さ** | 存在しない文献を引用することがある |

### 2.4.3 出力の検証方法

![AI出力の検証チェックリスト](/docs/images/01_入門/02_verification_checklist.jpeg)

---

## 2.5 マルチモーダルAI

### 2.5.1 マルチモーダルとは

**マルチモーダルAI**は、テキスト、画像、音声など複数の形式（モダリティ）を扱えるAIです。

![マルチモーダルAIの入出力例](/docs/images/01_入門/02_multimodal.jpeg)

### 2.5.2 代表的なマルチモーダルAI

| サービス | 対応モダリティ |
|----------|----------------|
| GPT-4o | テキスト、画像、音声 |
| Claude 3 | テキスト、画像 |
| Gemini | テキスト、画像、音声、動画 |
| DALL-E 3 | テキスト → 画像 |

---

## 2.6 AIの進化と今後

### 2.6.1 スケーリング則

![スケーリング則](/docs/images/01_入門/02_scaling.jpeg)

### 2.6.2 今後の展望

- **推論能力の向上**: より複雑な問題を解決できるAI
- **マルチモーダルの深化**: 様々なメディアをシームレスに扱う
- **エージェント化**: 自律的にタスクを実行するAI
- **効率化**: より少ないリソースで高性能なモデル
- **安全性向上**: 有害出力の防止、制御可能性の向上

---

## 確認クイズ

### Q1. Transformerの核心技術
Transformerアーキテクチャの核心となる技術は何ですか？

- [ ] A) 畳み込み処理
- [ ] B) 自己注意機構（Self-Attention）
- [ ] C) 再帰的処理
- [ ] D) ルールベース処理

### Q2. LLMの学習
LLMの事前学習で主に行われるタスクは何ですか？

- [ ] A) 質問に正確に回答する
- [ ] B) 次の単語を予測する
- [ ] C) 文法の正誤を判定する
- [ ] D) 感情を分析する

### Q3. ハルシネーション
ハルシネーションが起きる主な原因として正しいものは？

- [ ] A) AIが意図的に嘘をついている
- [ ] B) インターネット接続が不安定
- [ ] C) もっともらしい文章を生成するよう学習しており、事実の正確性は直接学習していない
- [ ] D) ユーザーの質問が曖昧だから

---

## 章末まとめ

### 本章のポイント

1. **Transformer**の自己注意機構により、文脈を理解した文章生成が可能に
2. LLMは**「次の単語を予測する」**タスクで大量のテキストから学習
3. **ハルシネーション**など、AIの出力には限界があり検証が必要
4. **マルチモーダルAI**により、テキスト以外のデータも扱えるように

### 次章への準備

次章「AI倫理と安全性」では、AI活用における倫理的課題と安全な利用方法を学びます。

---

## 参考資料

- Vaswani et al. "Attention Is All You Need" (2017)
- OpenAI "Language Models are Few-Shot Learners" (GPT-3論文)
- Anthropic "Constitutional AI" 技術資料

---

©健全AI教育協会
